{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Conv Vae Fmnist"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "import glob as glob\n",
        "import random\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras import regularizers\n",
        "from typeguard import typechecked\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download F_mnist data set from: https://www.kaggle.com/zalando-research/fashionmnist"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pd.read_csv(\"F_mnist/train.csv\")\n",
        "X_test = pd.read_csv(\"F_mnist/test.csv\")"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.iloc[:,2:].values\n",
        "X_valid = X_test.iloc[:,1:].values\n",
        "\n",
        "X_train, X_valid = X_train/255, X_valid/255"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.reshape(X_train.shape[0],28,28)\n",
        "X_valid = X_valid.reshape(X_valid.shape[0],28,28)"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(X_train[6,:,:], cmap=\"gray\")\n",
        "plt.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": "<Figure size 432x288 with 1 Axes>"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "execution_count": 22,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Model with fit()"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sampling(layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        mean, log_var = inputs\n",
        "        return tf.random.normal(tf.shape(log_var)) * tf.math.exp(log_var / 2) + mean\n",
        "    \n",
        "def rounded_accuracy(y_true, y_pred):\n",
        "    return keras.metrics.binary_accuracy(tf.round(y_true), tf.round(y_pred))    "
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class SpectralNormalization(tf.keras.layers.Wrapper):\n",
        "    \"\"\"Performs spectral normalization on weights.\n",
        "    This wrapper controls the Lipschitz constant of the layer by\n",
        "    constraining its spectral norm, which can stabilize the training of GANs.\n",
        "    See [Spectral Normalization for Generative Adversarial Networks](https://arxiv.org/abs/1802.05957).\n",
        "    ```python\n",
        "    net = SpectralNormalization(\n",
        "        tf.keras.layers.Conv2D(2, 2, activation=\"relu\"),\n",
        "        input_shape=(32, 32, 3))(x)\n",
        "    net = SpectralNormalization(\n",
        "        tf.keras.layers.Conv2D(16, 5, activation=\"relu\"))(net)\n",
        "    net = SpectralNormalization(\n",
        "        tf.keras.layers.Dense(120, activation=\"relu\"))(net)\n",
        "    net = SpectralNormalization(\n",
        "        tf.keras.layers.Dense(n_classes))(net)\n",
        "    ```\n",
        "    Arguments:\n",
        "      layer: A `tf.keras.layers.Layer` instance that\n",
        "        has either `kernel` or `embeddings` attribute.\n",
        "      power_iterations: `int`, the number of iterations during normalization.\n",
        "    Raises:\n",
        "      AssertionError: If not initialized with a `Layer` instance.\n",
        "      ValueError: If initialized with negative `power_iterations`.\n",
        "      AttributeError: If `layer` does not has `kernel` or `embeddings` attribute.\n",
        "    \"\"\"\n",
        "\n",
        "    @typechecked\n",
        "    def __init__(self, layer: tf.keras.layers, power_iterations: int = 10, **kwargs):\n",
        "        super().__init__(layer, **kwargs)\n",
        "        if power_iterations <= 0:\n",
        "            raise ValueError(\n",
        "                \"`power_iterations` should be greater than zero, got \"\n",
        "                \"`power_iterations={}`\".format(power_iterations)\n",
        "            )\n",
        "        self.power_iterations = power_iterations\n",
        "        self._initialized = False\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        \"\"\"Build `Layer`\"\"\"\n",
        "        super().build(input_shape)\n",
        "        input_shape = tf.TensorShape(input_shape)\n",
        "        self.input_spec = tf.keras.layers.InputSpec(shape=[None] + input_shape[1:])\n",
        "\n",
        "        if hasattr(self.layer, \"kernel\"):\n",
        "            self.w = self.layer.kernel\n",
        "        elif hasattr(self.layer, \"embeddings\"):\n",
        "            self.w = self.layer.embeddings\n",
        "        else:\n",
        "            raise AttributeError(\n",
        "                \"{} object has no attribute 'kernel' nor \"\n",
        "                \"'embeddings'\".format(type(self.layer).__name__)\n",
        "            )\n",
        "\n",
        "        self.w_shape = self.w.shape.as_list()\n",
        "\n",
        "        self.u = self.add_weight(\n",
        "            shape=(1, self.w_shape[-1]),\n",
        "            initializer=tf.initializers.TruncatedNormal(stddev=0.02),\n",
        "            trainable=False,\n",
        "            name=\"sn_u\",\n",
        "            dtype=self.w.dtype,\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        \"\"\"Call `Layer`\"\"\"\n",
        "        if training is None:\n",
        "            training = tf.keras.backend.learning_phase()\n",
        "\n",
        "        if training:\n",
        "            self.normalize_weights()\n",
        "\n",
        "        output = self.layer(inputs)\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return tf.TensorShape(self.layer.compute_output_shape(input_shape).as_list())\n",
        "\n",
        "    @tf.function\n",
        "    def normalize_weights(self):\n",
        "        \"\"\"Generate spectral normalized weights.\n",
        "        This method will update the value of `self.w` with the\n",
        "        spectral normalized value, so that the layer is ready for `call()`.\n",
        "        \"\"\"\n",
        "\n",
        "        w = tf.reshape(self.w, [-1, self.w_shape[-1]])\n",
        "        u = self.u\n",
        "\n",
        "        with tf.name_scope(\"spectral_normalize\"):\n",
        "            for _ in range(self.power_iterations):\n",
        "                v = tf.math.l2_normalize(tf.matmul(u, w, transpose_b=True))\n",
        "                u = tf.math.l2_normalize(tf.matmul(v, w))\n",
        "\n",
        "            sigma = tf.matmul(tf.matmul(v, w), u, transpose_b=True)\n",
        "\n",
        "            self.w.assign(self.w / sigma)\n",
        "            self.u.assign(u)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\"power_iterations\": self.power_iterations}\n",
        "        base_config = super().get_config()\n",
        "        return {**base_config, **config}"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(Model):\n",
        "    def train_step(self, data):\n",
        "        # Unpack the data. Its structure depends on your model and\n",
        "        # on what you pass to `fit()`.\n",
        "        x, y = data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)  # Forward pass\n",
        "            # Compute the loss value\n",
        "            # (the loss function is configured in `compile()`)\n",
        "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "\n",
        "        # Compute gradients\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "        # Update metrics (includes the metric that tracks the loss)\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "        # Return a dict mapping metric names to current value\n",
        "        return {m.name: m.result() for m in self.metrics}"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "codings_size = 20\n",
        "n_pixels = 28*28\n",
        "\n",
        "inputs = keras.layers.Input(shape=[28, 28,1])\n",
        "z = keras.layers.Conv2D(filters=12, kernel_size=3, strides=(2, 2),activation='relu')(inputs)\n",
        "z = keras.layers.Conv2D(filters=6, kernel_size=3, strides=(2, 2),activation='relu')(z)\n",
        "z = keras.layers.Flatten()(z)\n",
        "codings_mean = keras.layers.Dense(codings_size)(z)\n",
        "codings_log_var = keras.layers.Dense(codings_size)(z)\n",
        "codings = Sampling()([codings_mean, codings_log_var])\n",
        "encoder = keras.models.Model(\n",
        "inputs=[inputs], outputs=[codings_mean, codings_log_var, codings])\n",
        "print(encoder.summary())\n",
        "    \n",
        "# Declare decoder\n",
        "decoder_inputs = keras.layers.Input(shape=[codings_size])\n",
        "x = tf.keras.layers.Dense(units=196, activation=tf.nn.relu)(decoder_inputs)\n",
        "x = tf.keras.layers.Reshape(target_shape=(7, 7, 4))(x)\n",
        "x = tf.keras.layers.Conv2DTranspose(filters=6, kernel_size=3, strides=2, padding='same',\n",
        "                    activation='relu')(x)\n",
        "x = tf.keras.layers.Conv2DTranspose(filters=12, kernel_size=3, strides=2, padding='same',\n",
        "                    activation='relu')(x)\n",
        "outputs = SpectralNormalization(tf.keras.layers.Conv2DTranspose(activation='sigmoid',\n",
        "                    filters=1, kernel_size=3, strides=1, padding='same'))(x)\n",
        "decoder = keras.models.Model(inputs=[decoder_inputs], outputs=[outputs])\n",
        "print(decoder.summary())\n",
        "        \n",
        "# Declare VAE\n",
        "_, _, codings = encoder(inputs)\n",
        "reconstructions = decoder(codings)\n",
        "\n",
        "vae = VAE(inputs=[inputs], outputs=[reconstructions])  "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 28, 28, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 13, 13, 12)   120         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 6, 6, 6)      654         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 216)          0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 20)           4340        flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 20)           4340        flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "sampling (Sampling)             (None, 20)           0           dense[0][0]                      \n",
            "                                                                 dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 9,454\n",
            "Trainable params: 9,454\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 20)]              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 196)               4116      \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 7, 7, 4)           0         \n",
            "_________________________________________________________________\n",
            "conv2d_transpose (Conv2DTran (None, 14, 14, 6)         222       \n",
            "_________________________________________________________________\n",
            "conv2d_transpose_1 (Conv2DTr (None, 28, 28, 12)        660       \n",
            "_________________________________________________________________\n",
            "spectral_normalization (Spec (None, 28, 28, 1)         121       \n",
            "=================================================================\n",
            "Total params: 5,119\n",
            "Trainable params: 5,107\n",
            "Non-trainable params: 12\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "execution_count": 9,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latent_loss = -0.5 * tf.math.reduce_sum(\n",
        "    1 + codings_log_var - tf.math.exp(codings_log_var) - tf.math.square(codings_mean),\n",
        "    axis=-1)\n",
        "\n",
        "vae.add_loss(tf.math.reduce_mean(latent_loss) / n_pixels)\n",
        "vae.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[rounded_accuracy])\n",
        "history = vae.fit(X_train, X_train, epochs=25, batch_size=128,\n",
        "                             validation_data=(X_valid, X_valid))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "469/469 [==============================] - 18s 37ms/step - loss: 0.4386 - rounded_accuracy: 0.8007 - val_loss: 0.3812 - val_rounded_accuracy: 0.8644\n",
            "Epoch 2/25\n",
            "469/469 [==============================] - 19s 40ms/step - loss: 0.3669 - rounded_accuracy: 0.8792 - val_loss: 0.3553 - val_rounded_accuracy: 0.8899\n",
            "Epoch 3/25\n",
            "469/469 [==============================] - 24s 50ms/step - loss: 0.3514 - rounded_accuracy: 0.8923 - val_loss: 0.3450 - val_rounded_accuracy: 0.8978\n",
            "Epoch 4/25\n",
            "469/469 [==============================] - 28s 60ms/step - loss: 0.3435 - rounded_accuracy: 0.8979 - val_loss: 0.3399 - val_rounded_accuracy: 0.9009\n",
            "Epoch 5/25\n",
            "469/469 [==============================] - 27s 58ms/step - loss: 0.3390 - rounded_accuracy: 0.9016 - val_loss: 0.3354 - val_rounded_accuracy: 0.9047\n",
            "Epoch 6/25\n",
            "469/469 [==============================] - 23s 49ms/step - loss: 0.3361 - rounded_accuracy: 0.9040 - val_loss: 0.3348 - val_rounded_accuracy: 0.9051\n",
            "Epoch 7/25\n",
            "469/469 [==============================] - 22s 46ms/step - loss: 0.3341 - rounded_accuracy: 0.9056 - val_loss: 0.3319 - val_rounded_accuracy: 0.9083\n",
            "Epoch 8/25\n",
            "469/469 [==============================] - 21s 45ms/step - loss: 0.3326 - rounded_accuracy: 0.9069 - val_loss: 0.3315 - val_rounded_accuracy: 0.9104\n",
            "Epoch 9/25\n",
            "469/469 [==============================] - 21s 45ms/step - loss: 0.3315 - rounded_accuracy: 0.9079 - val_loss: 0.3307 - val_rounded_accuracy: 0.9070\n",
            "Epoch 10/25\n",
            "469/469 [==============================] - 22s 47ms/step - loss: 0.3306 - rounded_accuracy: 0.9085 - val_loss: 0.3293 - val_rounded_accuracy: 0.9088\n",
            "Epoch 11/25\n",
            "469/469 [==============================] - 21s 44ms/step - loss: 0.3299 - rounded_accuracy: 0.9092 - val_loss: 0.3281 - val_rounded_accuracy: 0.9113\n",
            "Epoch 12/25\n",
            "469/469 [==============================] - 21s 45ms/step - loss: 0.3293 - rounded_accuracy: 0.9098 - val_loss: 0.3282 - val_rounded_accuracy: 0.9110\n",
            "Epoch 13/25\n",
            "469/469 [==============================] - 22s 48ms/step - loss: 0.3288 - rounded_accuracy: 0.9102 - val_loss: 0.3279 - val_rounded_accuracy: 0.9096\n",
            "Epoch 14/25\n",
            "469/469 [==============================] - 21s 45ms/step - loss: 0.3284 - rounded_accuracy: 0.9107 - val_loss: 0.3275 - val_rounded_accuracy: 0.9138\n",
            "Epoch 15/25\n",
            "469/469 [==============================] - 20s 42ms/step - loss: 0.3280 - rounded_accuracy: 0.9111 - val_loss: 0.3278 - val_rounded_accuracy: 0.9089\n",
            "Epoch 16/25\n",
            "469/469 [==============================] - 20s 43ms/step - loss: 0.3276 - rounded_accuracy: 0.9113 - val_loss: 0.3278 - val_rounded_accuracy: 0.9121\n",
            "Epoch 17/25\n",
            "469/469 [==============================] - 19s 41ms/step - loss: 0.3273 - rounded_accuracy: 0.9116 - val_loss: 0.3280 - val_rounded_accuracy: 0.9118\n",
            "Epoch 18/25\n",
            "469/469 [==============================] - 20s 43ms/step - loss: 0.3270 - rounded_accuracy: 0.9118 - val_loss: 0.3255 - val_rounded_accuracy: 0.9135\n",
            "Epoch 19/25\n",
            "469/469 [==============================] - 20s 42ms/step - loss: 0.3267 - rounded_accuracy: 0.9121 - val_loss: 0.3248 - val_rounded_accuracy: 0.9139\n",
            "Epoch 20/25\n",
            "469/469 [==============================] - 20s 43ms/step - loss: 0.3264 - rounded_accuracy: 0.9124 - val_loss: 0.3243 - val_rounded_accuracy: 0.9135\n",
            "Epoch 21/25\n",
            "469/469 [==============================] - 21s 44ms/step - loss: 0.3261 - rounded_accuracy: 0.9126 - val_loss: 0.3254 - val_rounded_accuracy: 0.9157\n",
            "Epoch 22/25\n",
            "469/469 [==============================] - 21s 46ms/step - loss: 0.3260 - rounded_accuracy: 0.9127 - val_loss: 0.3244 - val_rounded_accuracy: 0.9131\n",
            "Epoch 23/25\n",
            "469/469 [==============================] - 21s 45ms/step - loss: 0.3257 - rounded_accuracy: 0.9128 - val_loss: 0.3238 - val_rounded_accuracy: 0.9138\n",
            "Epoch 24/25\n",
            "469/469 [==============================] - 21s 45ms/step - loss: 0.3256 - rounded_accuracy: 0.9132 - val_loss: 0.3236 - val_rounded_accuracy: 0.9144\n",
            "Epoch 25/25\n",
            "469/469 [==============================] - 21s 45ms/step - loss: 0.3253 - rounded_accuracy: 0.9133 - val_loss: 0.3243 - val_rounded_accuracy: 0.9140\n"
          ]
        }
      ],
      "execution_count": 10,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Sample"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "codings = tf.random.normal(shape=[1, codings_size])\n",
        "images = decoder(codings).numpy()\n",
        "plt.imshow(images[0,:,:,0], cmap=\"gray\")\n",
        "plt.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": "<Figure size 432x288 with 1 Axes>"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "execution_count": 40,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "images.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "(1, 28, 28, 1)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "0.26.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}